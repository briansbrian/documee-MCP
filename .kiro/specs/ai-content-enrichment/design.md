# AI Content Enrichment Design

## Overview

The AI Content Enrichment feature enhances the existing course generator by leveraging **Kiro (the AI assistant)** to generate rich, educational content. Instead of building LLM integration into the Python codebase, Kiro reads the basic course structure generated by MCP tools, analyzes the code and context, and writes enriched explanations, narratives, and exercises directly into the course files.

**Key Design Principle**: The MCP server generates basic course structure with placeholders, and Kiro enriches the content by reading code, understanding context, and writing educational narratives. This approach is simpler, more flexible, and leverages Kiro's existing capabilities.

## Architecture

### High-Level Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                    MCP Server (Python)                       │
│  ┌──────────────────────────────────────────────────────┐  │
│  │         Course Generator Tools                       │  │
│  │  - scan_codebase                                     │  │
│  │  - analyze_file                                      │  │
│  │  - export_course (generates basic structure)        │  │
│  │  - get_lesson_content (NEW - returns lesson data)   │  │
│  │  - update_lesson_content (NEW - saves enriched)     │  │
│  └──────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────┘
                            ▲
                            │ MCP Protocol
                            ▼
┌─────────────────────────────────────────────────────────────┐
│                    Kiro (AI Assistant)                       │
│  ┌──────────────────────────────────────────────────────┐  │
│  │         Content Enrichment Workflow                  │  │
│  │  1. Call export_course to generate basic structure  │  │
│  │  2. For each lesson:                                 │  │
│  │     - Call get_lesson_content                        │  │
│  │     - Read source code files                         │  │
│  │     - Analyze code patterns and context              │  │
│  │     - Generate rich explanations                     │  │
│  │     - Write learning narratives                      │  │
│  │     - Create detailed exercises with hints           │  │
│  │     - Call update_lesson_content with enriched data  │  │
│  └──────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────┘
```

### Integration Points

1. **MCP Tools**: Add new tools for reading/writing lesson content
2. **Kiro Workflow**: Implement enrichment workflow that Kiro follows
3. **Export Formats**: No changes needed - existing exporters already handle rich content fields

## Components and Interfaces

### 1. New MCP Tool: get_enrichment_guide

**Purpose**: Provides a comprehensive, evidence-based guide for AI to explain the project systematically.

**Location**: `src/server.py` (add new tool)

```python
@mcp.tool()
async def get_enrichment_guide(
    codebase_id: str,
    lesson_id: str
) -> Dict[str, Any]:
    """
    Generates a comprehensive enrichment guide for AI assistants
    
    This tool provides structured, evidence-based guidance following the
    Feature-to-Lesson Mapping and Knowledge-to-Course frameworks.
    
    Args:
        codebase_id: Codebase identifier from scan_codebase
        lesson_id: Lesson identifier (e.g., "module-1-lesson-1")
        
    Returns:
        Enrichment guide containing:
        
        1. feature_mapping:
           - feature_name: Name of the feature this lesson teaches
           - user_facing_purpose: What users do with this feature
           - business_value: Why this feature exists
           - entry_points: Where users/code interact with this feature
           - feature_flow: Step-by-step user/data flow
           
        2. evidence_bundle:
           - source_files: List of files with line ranges
           - test_files: Tests that validate behavior
           - git_commits: Relevant commits explaining "why"
           - documentation: Inline comments, JSDoc, README sections
           - dependencies: What this depends on (with evidence)
           - dependents: What depends on this (with evidence)
           
        3. validation_checklist:
           - code_behavior: What the code actually does (cite tests)
           - expected_behavior: What tests expect (cite test descriptions)
           - documentation_alignment: What docs say (cite comments)
           - git_context: Why it was built (cite commits)
           - consistency_check: Cross-file validation results
           
        4. teaching_value_assessment:
           - reusability_score: 0-3 (is this pattern reusable?)
           - best_practice_score: 0-3 (is this a best practice?)
           - fundamentality_score: 0-3 (is this fundamental?)
           - uniqueness_score: 0-2 (is this unique/interesting?)
           - junior_dev_score: 0-3 (valuable for junior devs?)
           - total_score: 0-14 (teach if > 7)
           - reasoning: Why these scores
           
        5. systematic_investigation:
           - what_it_does: Factual description (cite code)
           - why_it_exists: Business/technical reason (cite commits/docs)
           - how_it_works: Technical implementation (cite code sections)
           - when_its_used: Usage scenarios (cite call sites)
           - edge_cases: Special handling (cite tests)
           - common_pitfalls: Known issues (cite comments/tests)
           
        6. narrative_structure:
           - introduction_points: Context and motivation
           - learning_progression: Ordered concepts (simple → complex)
           - code_walkthrough_order: Which code to explain when
           - conclusion_points: Key takeaways
           - next_steps: What to learn next
           
        7. code_sections: List of code sections with evidence
           Each section:
           - file_path: Source file
           - line_range: [start, end]
           - code_snippet: Actual code
           - purpose: What it does (cite tests)
           - key_concepts: Concepts demonstrated
           - explanation_approach: How to explain (simple → complex)
           - related_code: Links to related sections (with context)
           - test_evidence: Tests that validate this code
           - git_evidence: Commits that explain why
           - common_mistakes: Pitfalls to highlight
           
        8. architecture_context:
           - component_role: Role in system (cite dependency graph)
           - data_flow: How data moves (cite code)
           - interaction_diagram: Mermaid diagram
           - dependencies: What it needs (cite imports)
           - dependents: What needs it (cite usage)
           - design_patterns: Patterns used (cite evidence)
           
        9. real_world_context:
           - practical_use_cases: Real scenarios
           - analogies: Beginner-friendly comparisons
           - industry_patterns: Standard approaches
           - best_practices: What to emphasize
           - anti_patterns: What to avoid
           
        10. exercise_generation:
            - hands_on_tasks: Progressive exercises
            - starter_code: Template to complete
            - solution_code: Complete solution (from codebase)
            - test_cases: Validation tests (from codebase)
            - progressive_hints: 3-5 hints (general → specific)
            - self_assessment: Questions to check understanding
            
        11. anti_hallucination_rules:
            - always_cite: "Never explain without citing evidence"
            - distinguish_fact_inference: "Mark inferences clearly"
            - validate_against_tests: "Check tests before explaining"
            - cross_reference: "Verify across multiple files"
            - avoid_assumptions: "Don't guess the 'why'"
            
        12. enrichment_instructions:
            - tone: "casual" (beginner-friendly)
            - depth: "detailed" (explain thoroughly)
            - focus_areas: What to emphasize
            - avoid_topics: What not to mention
            - evidence_requirements: "Cite for every claim"
    """
    
@mcp.tool()
async def update_lesson_content(
    codebase_id: str,
    lesson_id: str,
    enriched_content: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Updates lesson with enriched content from AI
    
    Args:
        codebase_id: Codebase identifier
        lesson_id: Lesson identifier
        enriched_content: Dictionary with enriched fields:
            - description: Rich description with context
            - content: Full learning narrative
            - code_examples: Enhanced with detailed explanations
            - exercises: Enhanced with hints and instructions
            - learning_objectives: Generated learning goals
            
    Returns:
        Success status and updated lesson metadata
    """
```

### 2. EnrichmentGuideGenerator

**Purpose**: Generates evidence-based enrichment guides following systematic investigation framework.

**Location**: `src/course/enrichment_guide_generator.py` (new)

```python
class EnrichmentGuideGenerator:
    """
    Generates comprehensive, evidence-based enrichment guides
    
    Implements the Feature-to-Lesson Mapping and Knowledge-to-Course frameworks
    to ensure AI assistants have rich, validated guidance for content enrichment.
    """
    
    def __init__(self, analysis_engine, course_generator, git_analyzer):
        self.analysis = analysis_engine
        self.course_gen = course_generator
        self.git = git_analyzer
        
    def generate_guide(
        self,
        codebase_id: str,
        lesson_id: str
    ) -> EnrichmentGuide:
        """
        Generates a comprehensive enrichment guide with evidence
        
        Process (following KNOWLEDGE-TO-COURSE-FRAMEWORK):
        1. Extract knowledge with evidence
        2. Validate against tests, docs, history
        3. Assess teaching value
        4. Build narrative structure
        5. Generate code section guides with citations
        6. Create exercises from actual code
        7. Package with anti-hallucination rules
        """
        lesson = self._get_lesson_data(codebase_id, lesson_id)
        file_analysis = self._get_file_analysis(lesson.source_files)
        
        # Phase 1: Feature Mapping
        feature_mapping = self._map_feature(lesson, file_analysis)
        
        # Phase 2: Evidence Collection
        evidence_bundle = self._collect_evidence(lesson, file_analysis)
        
        # Phase 3: Validation
        validation = self._validate_understanding(evidence_bundle)
        
        # Phase 4: Teaching Value Assessment
        teaching_value = self._assess_teaching_value(
            feature_mapping,
            evidence_bundle,
            file_analysis
        )
        
        # Phase 5: Systematic Investigation
        investigation = self._systematic_investigation(
            feature_mapping,
            evidence_bundle,
            validation
        )
        
        # Phase 6: Narrative Structure
        narrative = self._build_narrative_structure(
            investigation,
            teaching_value
        )
        
        # Phase 7: Code Section Guides
        code_sections = self._generate_code_section_guides(
            lesson,
            evidence_bundle,
            investigation
        )
        
        # Phase 8: Architecture Context
        architecture = self._extract_architecture_context(
            lesson,
            evidence_bundle
        )
        
        # Phase 9: Real-World Context
        real_world = self._suggest_real_world_context(
            feature_mapping,
            file_analysis.patterns
        )
        
        # Phase 10: Exercise Generation
        exercises = self._generate_exercises(
            feature_mapping,
            evidence_bundle,
            teaching_value
        )
        
        return EnrichmentGuide(
            lesson_id=lesson_id,
            feature_mapping=feature_mapping,
            evidence_bundle=evidence_bundle,
            validation_checklist=validation,
            teaching_value_assessment=teaching_value,
            systematic_investigation=investigation,
            narrative_structure=narrative,
            code_sections=code_sections,
            architecture_context=architecture,
            real_world_context=real_world,
            exercise_generation=exercises,
            anti_hallucination_rules=self._get_anti_hallucination_rules(),
            enrichment_instructions=self._get_enrichment_instructions()
        )
        
    def _map_feature(
        self,
        lesson: LessonContent,
        file_analysis: FileAnalysis
    ) -> FeatureMapping:
        """
        Maps code to user-facing feature (FEATURE-TO-LESSON-MAPPING)
        
        Investigates:
        - What feature does this code implement?
        - What do users do with this feature?
        - Why does this feature exist?
        - Where do users interact with it?
        - How does the feature flow work?
        """
        # Identify feature from routes, components, API endpoints
        # Trace user flow through the code
        # Extract business value from comments/docs
        
    def _collect_evidence(
        self,
        lesson: LessonContent,
        file_analysis: FileAnalysis
    ) -> EvidenceBundle:
        """
        Collects all evidence for validation (anti-hallucination)
        
        Evidence sources:
        - Source code files with line numbers
        - Test files that validate behavior
        - Git commits explaining "why"
        - Inline comments and documentation
        - Dependency relationships
        - Usage examples (call sites)
        """
        return EvidenceBundle(
            source_files=self._get_source_files_with_lines(lesson),
            test_files=self._find_related_tests(lesson),
            git_commits=self.git.get_relevant_commits(lesson.source_files),
            documentation=self._extract_documentation(file_analysis),
            dependencies=self._get_dependencies_with_evidence(file_analysis),
            dependents=self._get_dependents_with_evidence(file_analysis)
        )
        
    def _validate_understanding(
        self,
        evidence: EvidenceBundle
    ) -> ValidationChecklist:
        """
        Validates understanding against multiple sources
        
        Validation steps (KNOWLEDGE-TO-COURSE-FRAMEWORK):
        1. What does code do? (read code)
        2. What do tests expect? (read tests)
        3. What does documentation say? (read docs)
        4. Why does it exist? (read git history)
        5. Is understanding consistent? (cross-reference)
        """
        return ValidationChecklist(
            code_behavior=self._analyze_code_behavior(evidence.source_files),
            expected_behavior=self._analyze_test_expectations(evidence.test_files),
            documentation_alignment=self._check_doc_alignment(evidence.documentation),
            git_context=self._extract_git_context(evidence.git_commits),
            consistency_check=self._cross_reference_sources(evidence)
        )
        
    def _assess_teaching_value(
        self,
        feature: FeatureMapping,
        evidence: EvidenceBundle,
        analysis: FileAnalysis
    ) -> TeachingValueAssessment:
        """
        Scores teaching value (0-14 scale)
        
        Criteria:
        - Reusability: 0-3 (is pattern reusable?)
        - Best practice: 0-3 (follows best practices?)
        - Fundamentality: 0-3 (is it fundamental?)
        - Uniqueness: 0-2 (interesting/unique?)
        - Junior dev value: 0-3 (helps junior devs?)
        
        Teach if total > 7
        """
        scores = {
            "reusability": self._score_reusability(analysis.patterns),
            "best_practice": self._score_best_practice(evidence),
            "fundamentality": self._score_fundamentality(feature),
            "uniqueness": self._score_uniqueness(analysis),
            "junior_dev": self._score_junior_dev_value(feature, analysis)
        }
        
        total = sum(scores.values())
        
        return TeachingValueAssessment(
            scores=scores,
            total_score=total,
            should_teach=total > 7,
            reasoning=self._explain_scores(scores)
        )
        
    def _systematic_investigation(
        self,
        feature: FeatureMapping,
        evidence: EvidenceBundle,
        validation: ValidationChecklist
    ) -> SystematicInvestigation:
        """
        Systematic investigation of the code
        
        Questions answered:
        - What does it do? (factual, cite code)
        - Why does it exist? (cite commits/docs)
        - How does it work? (cite code sections)
        - When is it used? (cite call sites)
        - What are edge cases? (cite tests)
        - What are pitfalls? (cite comments/tests)
        """
        return SystematicInvestigation(
            what_it_does=self._describe_functionality(evidence, validation),
            why_it_exists=self._explain_purpose(feature, evidence.git_commits),
            how_it_works=self._explain_implementation(evidence.source_files),
            when_its_used=self._find_usage_scenarios(evidence.dependents),
            edge_cases=self._extract_edge_cases(evidence.test_files),
            common_pitfalls=self._identify_pitfalls(evidence)
        )
        
    def _generate_code_section_guides(
        self,
        lesson: LessonContent,
        evidence: EvidenceBundle,
        investigation: SystematicInvestigation
    ) -> List[CodeSectionGuide]:
        """
        Generates guides for each code section with full evidence
        
        Each guide includes:
        - Code snippet with file/line references
        - Purpose (what it does) with test citations
        - Key concepts demonstrated
        - Explanation approach (simple → complex)
        - Related code with context
        - Test evidence validating behavior
        - Git evidence explaining why
        - Common mistakes to highlight
        """
        sections = []
        
        for code_example in lesson.code_examples:
            # Find related tests
            tests = self._find_tests_for_code(code_example, evidence.test_files)
            
            # Find git context
            commits = self._find_commits_for_code(code_example, evidence.git_commits)
            
            # Find related code
            related = self._find_related_code(code_example, evidence)
            
            sections.append(CodeSectionGuide(
                file_path=code_example.file_path,
                line_range=(code_example.line_start, code_example.line_end),
                code_snippet=code_example.code,
                purpose=self._describe_purpose_with_evidence(code_example, tests),
                key_concepts=self._extract_concepts(code_example),
                explanation_approach=self._suggest_explanation_approach(code_example),
                related_code=related,
                test_evidence=tests,
                git_evidence=commits,
                common_mistakes=self._identify_mistakes(code_example, tests)
            ))
            
        return sections
        
    def _generate_exercises(
        self,
        feature: FeatureMapping,
        evidence: EvidenceBundle,
        teaching_value: TeachingValueAssessment
    ) -> ExerciseGeneration:
        """
        Generates exercises from actual codebase
        
        Process:
        1. Extract core functionality
        2. Create simplified version
        3. Provide starter code (from codebase)
        4. Define requirements (from tests)
        5. Create solution (actual code)
        6. Generate progressive hints
        """
        # Extract actual code as solution
        solution_code = self._extract_solution_code(evidence.source_files)
        
        # Simplify for starter code
        starter_code = self._create_starter_code(solution_code)
        
        # Extract requirements from tests
        requirements = self._extract_requirements_from_tests(evidence.test_files)
        
        # Generate progressive hints
        hints = self._generate_progressive_hints(solution_code, requirements)
        
        return ExerciseGeneration(
            hands_on_tasks=self._create_hands_on_tasks(feature),
            starter_code=starter_code,
            solution_code=solution_code,
            test_cases=evidence.test_files,
            progressive_hints=hints,
            self_assessment=self._create_assessment_questions(feature, evidence)
        )
```

### 3. Data Models for Enrichment Guides

**Purpose**: Comprehensive data models following the investigation frameworks.

**Location**: `src/course/enrichment_models.py` (new)

```python
@dataclass
class FeatureMapping:
    """Maps code to user-facing feature"""
    feature_name: str
    user_facing_purpose: str  # What users do
    business_value: str  # Why it exists
    entry_points: List[str]  # Where users interact
    feature_flow: List[str]  # Step-by-step flow

@dataclass
class EvidenceBundle:
    """Collection of evidence for validation"""
    source_files: List[Dict[str, Any]]  # {path, lines, code}
    test_files: List[Dict[str, Any]]  # {path, tests, descriptions}
    git_commits: List[Dict[str, Any]]  # {hash, message, date, author}
    documentation: List[Dict[str, Any]]  # {type, content, location}
    dependencies: List[Dict[str, Any]]  # {name, reason, evidence}
    dependents: List[Dict[str, Any]]  # {name, usage, evidence}

@dataclass
class ValidationChecklist:
    """Validation results from multiple sources"""
    code_behavior: str  # What code actually does
    expected_behavior: str  # What tests expect
    documentation_alignment: str  # What docs say
    git_context: str  # Why it was built
    consistency_check: bool  # Are all sources consistent?

@dataclass
class TeachingValueAssessment:
    """Teaching value scoring (0-14)"""
    scores: Dict[str, int]  # reusability, best_practice, etc.
    total_score: int  # Sum of all scores
    should_teach: bool  # True if total > 7
    reasoning: str  # Explanation of scores

@dataclass
class SystematicInvestigation:
    """Results of systematic code investigation"""
    what_it_does: str  # Factual description (cite code)
    why_it_exists: str  # Business/technical reason (cite commits)
    how_it_works: str  # Implementation details (cite code)
    when_its_used: List[str]  # Usage scenarios (cite call sites)
    edge_cases: List[str]  # Special handling (cite tests)
    common_pitfalls: List[str]  # Known issues (cite evidence)

@dataclass
class NarrativeStructure:
    """Structure for lesson narrative"""
    introduction_points: List[str]  # Context and motivation
    learning_progression: List[str]  # Ordered concepts (simple → complex)
    code_walkthrough_order: List[str]  # Which code to explain when
    conclusion_points: List[str]  # Key takeaways
    next_steps: List[str]  # What to learn next

@dataclass
class CodeSectionGuide:
    """Evidence-based guide for code section"""
    file_path: str
    line_range: Tuple[int, int]
    code_snippet: str
    purpose: str  # What it does (cite tests)
    key_concepts: List[str]  # Concepts demonstrated
    explanation_approach: List[str]  # How to explain (simple → complex)
    related_code: List[Dict[str, str]]  # {path, context, relationship}
    test_evidence: List[Dict[str, str]]  # {test_name, description, file}
    git_evidence: List[Dict[str, str]]  # {commit, message, date}
    common_mistakes: List[str]  # Pitfalls to highlight

@dataclass
class ArchitectureContext:
    """Architectural context with evidence"""
    component_role: str  # Role in system (cite dependency graph)
    data_flow: str  # How data moves (cite code)
    interaction_diagram: str  # Mermaid diagram
    dependencies: List[Dict[str, str]]  # {name, reason, evidence}
    dependents: List[Dict[str, str]]  # {name, usage, evidence}
    design_patterns: List[Dict[str, str]]  # {pattern, evidence, explanation}

@dataclass
class RealWorldContext:
    """Real-world context and analogies"""
    practical_use_cases: List[str]  # Real scenarios
    analogies: List[str]  # Beginner-friendly comparisons
    industry_patterns: List[str]  # Standard approaches
    best_practices: List[str]  # What to emphasize
    anti_patterns: List[str]  # What to avoid

@dataclass
class ExerciseGeneration:
    """Exercise generation from codebase"""
    hands_on_tasks: List[Dict[str, str]]  # {title, description, difficulty}
    starter_code: str  # Template to complete
    solution_code: str  # Complete solution (from codebase)
    test_cases: List[Dict[str, Any]]  # Validation tests
    progressive_hints: List[str]  # 3-5 hints (general → specific)
    self_assessment: List[str]  # Questions to check understanding

@dataclass
class AntiHallucinationRules:
    """Rules to prevent hallucination"""
    always_cite: str  # "Never explain without citing evidence"
    distinguish_fact_inference: str  # "Mark inferences clearly"
    validate_against_tests: str  # "Check tests before explaining"
    cross_reference: str  # "Verify across multiple files"
    avoid_assumptions: str  # "Don't guess the 'why'"

@dataclass
class EnrichmentInstructions:
    """Instructions for AI enrichment"""
    tone: str  # casual (beginner-friendly)
    depth: str  # detailed (explain thoroughly)
    focus_areas: List[str]  # What to emphasize
    avoid_topics: List[str]  # What not to mention
    evidence_requirements: str  # "Cite for every claim"

@dataclass
class EnrichmentGuide:
    """Complete enrichment guide with evidence"""
    lesson_id: str
    feature_mapping: FeatureMapping
    evidence_bundle: EvidenceBundle
    validation_checklist: ValidationChecklist
    teaching_value_assessment: TeachingValueAssessment
    systematic_investigation: SystematicInvestigation
    narrative_structure: NarrativeStructure
    code_sections: List[CodeSectionGuide]
    architecture_context: ArchitectureContext
    real_world_context: RealWorldContext
    exercise_generation: ExerciseGeneration
    anti_hallucination_rules: AntiHallucinationRules
    enrichment_instructions: EnrichmentInstructions
```

## Data Models

**No changes required** to existing models. The current models already support rich content:

### Existing LessonContent Model
```python
@dataclass
class LessonContent:
    title: str
    description: str  # ← AI enriches this
    content: str  # ← AI enriches this
    code_examples: List[CodeExample]
    exercises: List[Exercise]
    learning_objectives: List[str]  # ← AI enriches this
    key_concepts: List[str]
    estimated_duration_minutes: int
```

### Existing CodeExample Model
```python
@dataclass
class CodeExample:
    code: str
    language: str
    explanation: str  # ← AI enriches this
    file_path: str
    line_start: int
    line_end: int
```

### Existing Exercise Model
```python
@dataclass
class Exercise:
    title: str
    description: str  # ← AI enriches this
    instructions: str  # ← AI enriches this
    starter_code: str
    solution_code: str
    hints: List[str]  # ← AI generates these
    difficulty: str
```

## Kiro Enrichment Instructions

### System Prompt Addition

When Kiro is asked to enrich course content, it should follow these guidelines:

**Beginner-Friendly Defaults:**
- Use simple, accessible language
- Define all technical terms
- Avoid assumptions about prior knowledge
- Provide step-by-step explanations
- Use real-world analogies

**Content Structure:**
1. **Introduction**: Set context and learning goals (2-3 paragraphs)
2. **Progressive Explanation**: Build from simple to complex concepts
3. **Code Walkthrough**: Explain code in plain language with context
4. **Real-World Examples**: Show practical applications
5. **Summary**: Reinforce key takeaways

**Exercise Enhancement:**
- Clear, numbered instructions
- 3-5 progressive hints (start general, get more specific)
- Expected outcomes
- Common pitfalls to avoid

## Error Handling

### MCP Tool Error Handling

```python
@mcp.tool()
async def update_lesson_content(codebase_id, lesson_id, enriched_content):
    """Updates lesson with enriched content"""
    try:
        # Validate enriched content structure
        _validate_enriched_content(enriched_content)
        
        # Update lesson in course data
        course_data = _load_course_data(codebase_id)
        lesson = _find_lesson(course_data, lesson_id)
        
        if not lesson:
            return {
                "success": False,
                "error": f"Lesson {lesson_id} not found"
            }
        
        # Merge enriched content
        lesson.update(enriched_content)
        _save_course_data(codebase_id, course_data)
        
        return {
            "success": True,
            "lesson_id": lesson_id,
            "updated_fields": list(enriched_content.keys())
        }
        
    except ValidationError as e:
        return {
            "success": False,
            "error": f"Invalid content structure: {e}"
        }
    except Exception as e:
        logger.error(f"Error updating lesson: {e}")
        return {
            "success": False,
            "error": str(e)
        }

def _validate_enriched_content(content: Dict[str, Any]):
    """Validates enriched content structure"""
    required_fields = ["description", "content"]
    for field in required_fields:
        if field not in content or not content[field]:
            raise ValidationError(f"Missing or empty required field: {field}")
    
    # Validate code examples if present
    if "code_examples" in content:
        for example in content["code_examples"]:
            if "explanation" not in example:
                raise ValidationError("Code example missing explanation")
    
    # Validate exercises if present
    if "exercises" in content:
        for exercise in content["exercises"]:
            if "instructions" not in exercise or "hints" not in exercise:
                raise ValidationError("Exercise missing instructions or hints")
```

## Testing Strategy

### Unit Tests

1. **MCP Tool Tests**
   - Test `get_lesson_content` returns correct structure
   - Test `update_lesson_content` validates and saves correctly
   - Test `list_lessons_for_enrichment` returns proper lesson list
   - Verify error handling for invalid inputs

2. **Content Validation Tests**
   - Test validation of enriched content structure
   - Verify required fields are present
   - Test handling of malformed data

### Integration Tests

1. **End-to-End Enrichment Workflow**
   - Generate basic course with `export_course`
   - Retrieve lesson with `get_lesson_content`
   - Simulate Kiro enrichment
   - Update lesson with `update_lesson_content`
   - Verify enriched content persists correctly

2. **Export Tests**
   - Generate enriched course
   - Export to MkDocs format
   - Verify enriched content renders correctly
   - Test JSON export includes all enriched fields

### Manual Testing with Kiro

1. **Enrichment Quality**
   - Ask Kiro to enrich a sample lesson
   - Verify explanations are beginner-friendly
   - Check for appropriate analogies and examples
   - Validate exercise hints are progressive

2. **Workflow Testing**
   - Test full enrichment workflow from start to finish
   - Verify Kiro can read source files and understand context
   - Check that enriched content matches requirements

## Performance Considerations

### Enrichment Status Tracking

```python
@dataclass
class EnrichmentStatus:
    """Tracks enrichment progress for lessons"""
    lesson_id: str
    status: str  # "pending", "in_progress", "completed"
    enriched_at: Optional[datetime]
    enriched_by: str  # "kiro" or user identifier
    version: int  # Increment when re-enriched

# Store in course metadata
def _update_enrichment_status(codebase_id: str, lesson_id: str, status: str):
    """Updates enrichment status for a lesson"""
    course_data = _load_course_data(codebase_id)
    if "enrichment_status" not in course_data:
        course_data["enrichment_status"] = {}
    
    course_data["enrichment_status"][lesson_id] = {
        "status": status,
        "enriched_at": datetime.now().isoformat(),
        "version": course_data["enrichment_status"].get(lesson_id, {}).get("version", 0) + 1
    }
    _save_course_data(codebase_id, course_data)
```

### Incremental Enrichment

- Track which lessons have been enriched
- Allow re-enrichment when source code changes
- Support partial enrichment (enrich only specific lessons)
- Provide progress tracking for large courses

### Optimization Tips for Kiro

When enriching courses:
1. **Batch similar lessons**: Enrich lessons from the same module together to maintain context
2. **Prioritize high-value lessons**: Use teaching value scores to prioritize
3. **Reuse patterns**: When you see similar code patterns, adapt previous explanations
4. **Progressive enrichment**: Start with core lessons, then expand to advanced topics

## Example Enrichment Guide

### Sample Guide Output from get_enrichment_guide

```json
{
  "lesson_id": "module-2-lesson-3",
  "lesson_overview": {
    "title": "User Authentication System",
    "topic": "Secure user login and session management",
    "difficulty": "intermediate",
    "estimated_duration": 45,
    "prerequisites": ["Basic Python", "HTTP concepts", "Database basics"]
  },
  
  "narrative_structure": {
    "introduction_points": [
      "Authentication is critical for protecting user data",
      "This lesson covers password hashing and session tokens",
      "You'll learn industry-standard security practices"
    ],
    "learning_progression": [
      "Understanding authentication vs authorization",
      "Password hashing with bcrypt",
      "Session token generation",
      "Secure session storage",
      "Login flow implementation",
      "Common security vulnerabilities"
    ],
    "conclusion_points": [
      "Never store passwords in plain text",
      "Always use secure session tokens",
      "Implement rate limiting for protection"
    ]
  },
  
  "code_sections": [
    {
      "file_path": "src/auth/login.py",
      "line_range": [15, 35],
      "code_snippet": "def login(username, password):\n    user = db.get_user(username)\n    if not user:\n        return None\n    if bcrypt.checkpw(password, user.password_hash):\n        return create_session(user)\n    return None",
      "purpose": "Validates user credentials and creates a session",
      "key_concepts": [
        "Password verification",
        "Secure hashing comparison",
        "Session creation",
        "Error handling"
      ],
      "explanation_hints": [
        "Start by explaining why we don't check if password is correct first",
        "Emphasize timing attack prevention",
        "Explain bcrypt.checkpw vs simple comparison",
        "Walk through the happy path and error cases",
        "Connect to session creation in next section"
      ],
      "related_sections": [
        "src/auth/session.py:create_session",
        "src/models/user.py:User.password_hash"
      ],
      "common_pitfalls": [
        "Using == to compare passwords (timing attacks)",
        "Returning different errors for invalid username vs password",
        "Not rate limiting login attempts"
      ],
      "complexity_level": "moderate"
    },
    {
      "file_path": "src/auth/session.py",
      "line_range": [8, 20],
      "code_snippet": "def create_session(user):\n    token = jwt.encode({'user_id': user.id}, SECRET_KEY)\n    session = Session(token=token, user_id=user.id)\n    db.save(session)\n    return token",
      "purpose": "Creates a JWT session token for authenticated user",
      "key_concepts": [
        "JWT tokens",
        "Session storage",
        "Token-based authentication"
      ],
      "explanation_hints": [
        "Explain what JWT is and why it's useful",
        "Describe the token payload structure",
        "Discuss why we store sessions in database",
        "Mention token expiration (even though not shown here)"
      ],
      "related_sections": [
        "src/auth/middleware.py:verify_token"
      ],
      "common_pitfalls": [
        "Not setting token expiration",
        "Storing sensitive data in JWT payload",
        "Not invalidating tokens on logout"
      ],
      "complexity_level": "simple"
    }
  ],
  
  "architecture_context": {
    "component_role": "Handles user authentication and session management for the application",
    "dependencies": [
      "Database layer (src/db/)",
      "User model (src/models/user.py)",
      "bcrypt library for password hashing",
      "PyJWT for token generation"
    ],
    "dependents": [
      "API routes (src/api/routes.py)",
      "Authentication middleware (src/auth/middleware.py)",
      "User profile endpoints"
    ],
    "data_flow": "User credentials → Login validation → Password check → Session creation → Token return → Client storage",
    "interaction_diagram": "```mermaid\nsequenceDiagram\n    Client->>API: POST /login\n    API->>AuthService: login(username, password)\n    AuthService->>Database: get_user(username)\n    Database-->>AuthService: User object\n    AuthService->>AuthService: bcrypt.checkpw()\n    AuthService->>SessionService: create_session(user)\n    SessionService->>Database: save(session)\n    SessionService-->>AuthService: token\n    AuthService-->>API: token\n    API-->>Client: {token: 'jwt...'}\n```"
  },
  
  "real_world_context": {
    "use_cases": [
      "Web application login systems",
      "Mobile app authentication",
      "API authentication for third-party integrations",
      "Single sign-on (SSO) systems"
    ],
    "analogies": [
      "Password hashing is like a one-way safe - you can put things in but can't get them back out, only verify if what you put in matches",
      "Session tokens are like movie tickets - they prove you paid (authenticated) without needing to show your credit card every time",
      "Rate limiting is like a bank vault time delay - even if someone has the combination, they can't try it repeatedly"
    ],
    "industry_patterns": [
      "Token-based authentication (JWT)",
      "Password hashing with bcrypt/argon2",
      "Session management patterns",
      "Defense in depth security"
    ],
    "best_practices": [
      "Use bcrypt or argon2 for password hashing",
      "Implement rate limiting on login endpoints",
      "Use HTTPS for all authentication requests",
      "Set appropriate token expiration times",
      "Implement refresh token rotation"
    ]
  },
  
  "exercise_suggestions": {
    "hands_on_tasks": [
      {
        "title": "Add Password Strength Validation",
        "description": "Implement a function that validates password strength (length, special chars, etc.) before hashing"
      },
      {
        "title": "Implement Token Refresh",
        "description": "Add a refresh token mechanism to extend sessions without requiring re-login"
      },
      {
        "title": "Add Rate Limiting",
        "description": "Implement rate limiting to prevent brute force attacks (max 5 attempts per minute)"
      }
    ],
    "challenge_ideas": [
      {
        "title": "Multi-Factor Authentication",
        "description": "Extend the system to support 2FA using TOTP (Time-based One-Time Password)"
      },
      {
        "title": "OAuth Integration",
        "description": "Add OAuth2 support for login with Google/GitHub"
      }
    ],
    "self_assessment": [
      "Why do we use bcrypt instead of SHA256 for password hashing?",
      "What security vulnerability does timing-safe comparison prevent?",
      "Why should JWT tokens have an expiration time?",
      "What's the difference between authentication and authorization?"
    ]
  },
  
  "enrichment_instructions": {
    "tone": "casual",
    "depth": "detailed",
    "focus_areas": [
      "Security implications of each design choice",
      "Why certain approaches are better than alternatives",
      "Common mistakes and how to avoid them",
      "Real-world applications of these patterns"
    ],
    "avoid": [
      "Overly academic security theory",
      "Cryptography mathematics",
      "Framework-specific implementation details",
      "Deprecated security practices"
    ]
  }
}
```

### How AI Uses This Guide

When Kiro receives this guide, it:
1. Follows the narrative structure to create a coherent lesson flow
2. Uses code section guides to write detailed explanations
3. Incorporates real-world analogies and use cases
4. Creates exercises based on suggestions
5. Maintains the specified tone and depth
6. Emphasizes focus areas and avoids mentioned topics

## Implementation Phases

### Phase 1: MCP Tools (Week 1)
- Implement `get_lesson_content` tool
- Implement `update_lesson_content` tool
- Implement `list_lessons_for_enrichment` tool
- Add enrichment status tracking to course data model

### Phase 2: Kiro Workflow (Week 2)
- Create enrichment steering file with guidelines
- Test manual enrichment workflow with Kiro
- Refine enrichment instructions based on output quality
- Document best practices for enrichment

### Phase 3: Testing & Validation (Week 3)
- Write unit tests for MCP tools
- Test end-to-end enrichment workflow
- Validate enriched content quality
- Test export formats with enriched content

### Phase 4: Optimization (Week 4)
- Add batch enrichment support
- Implement progress tracking
- Add re-enrichment detection (when code changes)
- Create enrichment quality metrics

## Security Considerations

1. **Content Validation**
   - Validate enriched content structure before saving
   - Sanitize any user-provided input
   - Ensure code examples are syntactically valid
   - Check for appropriate content (no malicious code in examples)

2. **Data Integrity**
   - Validate lesson IDs to prevent path traversal
   - Ensure enriched content doesn't break existing course structure
   - Maintain version history for rollback capability

3. **Access Control**
   - MCP tools should validate codebase_id exists
   - Prevent unauthorized modification of course content
   - Log all enrichment operations for audit trail

## Success Metrics

1. **Content Quality**
   - Readability scores for enriched text (target: 8th-10th grade level for beginners)
   - Presence of required elements (analogies, examples, hints)
   - Code explanation completeness

2. **Enrichment Coverage**
   - Percentage of lessons enriched
   - Average enrichment time per lesson
   - Number of re-enrichments needed

3. **User Experience**
   - User feedback on enriched content quality
   - Comparison of enriched vs. basic course exports
   - Time saved vs. manual content creation

## Design Decisions

### Why Kiro-Driven Instead of LLM API Integration?

1. **Simplicity**: No need to integrate OpenAI/Anthropic SDKs into Python codebase
2. **Flexibility**: Kiro can adapt enrichment style based on conversation context
3. **Cost**: No additional API costs beyond existing Kiro usage
4. **Iteration**: Easier to refine enrichment quality through steering files
5. **Context**: Kiro has full access to codebase and can read any related files

### Trade-offs

**Advantages:**
- Simpler implementation (just MCP tools, no LLM integration)
- More flexible and conversational enrichment process
- Easier to customize and iterate on enrichment quality
- No additional API costs or rate limiting concerns

**Disadvantages:**
- Requires Kiro interaction (not fully automated)
- Enrichment quality depends on Kiro's capabilities
- May be slower for large courses (sequential processing)

**Mitigation:**
- Provide clear enrichment guidelines in steering files
- Support batch enrichment workflows
- Track enrichment status to enable resumption
